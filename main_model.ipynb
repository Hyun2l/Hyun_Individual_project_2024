{
 "cells": [
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T14:32:08.312850Z",
     "start_time": "2024-05-28T14:32:08.311560Z"
    }
   },
   "id": "66c67c9ab65da96b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('f1_2019_to_2023_all_drivers_all_data.csv', low_memory=False)\n",
    "\n",
    "# Convert time columns to seconds\n",
    "time_columns = ['LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time']\n",
    "for col in time_columns:\n",
    "    df[col] = pd.to_timedelta(df[col]).dt.total_seconds()\n",
    "\n",
    "# Convert binary columns to integer type\n",
    "df['Rainfall'] = df['Rainfall'].astype(int)\n",
    "df['FreshTyre'] = df['FreshTyre'].astype(int)\n",
    "df['IsAccurate'] = df['IsAccurate'].astype(int)\n",
    "\n",
    "\n",
    "# Categorize weather condition based on centroid values of Kmeans clustering\n",
    "def categorize_weather(row):\n",
    "    if row['Rainfall'] > 0:\n",
    "        return 'Rainy'\n",
    "    elif row['AirTemp'] > 28.43213126:\n",
    "        return 'high'\n",
    "    elif row['AirTemp'] > 21.31279265:\n",
    "        return 'medium'\n",
    "    elif row['AirTemp'] > 12.84901403:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'very_low'\n",
    "df['Weather_Category'] = df.apply(categorize_weather, axis=1)\n",
    "df = pd.get_dummies(df, columns=['Weather_Category'])\n",
    "\n",
    "\n",
    "# Create Track temperature category based on the result of Kmeans clustering \n",
    "df['TrackTemp_Cat'] = pd.cut(df['TrackTemp'], bins=[0, 18.96764999, 27.87457484, 35.04425766, 41.75142602, 50.51006013, 53.02449646], labels=['VERY_LOW', 'Low', 'Medium', 'Warm', 'High','VERY_High'])\n",
    "df = pd.get_dummies(df, columns=['TrackTemp_Cat'])\n",
    "\n",
    "\n",
    "# One-hot encoding\n",
    "df = pd.get_dummies(df, columns=['Driver', 'Compound', 'Team','TrackStatus','Circuit'])\n",
    "# Drop irrelevant columns\n",
    "columns_to_drop = ['Time', 'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime',\n",
    "                   'PitOutTime', 'PitInTime', 'LapStartDate', 'Deleted', 'DeletedReason', 'FastF1Generated',\n",
    "                   'IsPersonalBest', 'Sector3Time','LapStartTime','Sector2Time','Sector1Time']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "## Separate Rainy / dry days ##\n",
    "# 1. Separate LapTime as dry or wet(rainy) condition ( since lapTime of rainy day would be recognized as outliers)\n",
    "# 2. Remove Outliers for dry condition LapTime\n",
    "# 3. Build Combined LapTime df (Outliers for dry days are deleted)\n",
    "\n",
    "# Flag for rainy conditions\n",
    "df['IsRainy'] = df['Rainfall'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Separate dataframes for dry and wet conditions\n",
    "df_dry = df[df['IsRainy'] == 0]\n",
    "df_wet = df[df['IsRainy'] == 1]\n",
    "\n",
    "\n",
    "def remove_outliers(df, column_name, multiplier=1.5):\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    return df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
    "\n",
    "# Apply standard IQR for dry days\n",
    "df_dry_filtered = remove_outliers(df_dry, 'LapTime', multiplier=1.5)\n",
    "\n",
    "# Apply a more lenient IQR for wet days\n",
    "df_wet_filtered = remove_outliers(df_wet, 'LapTime', multiplier=2.0)\n",
    "\n",
    "\n",
    "df_combined = pd.concat([df_dry_filtered, df_wet_filtered], ignore_index=True)\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "X = df_combined.drop('LapTime', axis=1)\n",
    "y = df_combined['LapTime']\n",
    "\n",
    "\n",
    "train_years = [2019,2020,2021,2022]\n",
    "test_year = 2023\n",
    "# Split data based on year\n",
    "X_train = df_combined[df_combined['Year'].isin(train_years)].drop(['LapTime', 'Year'], axis=1)\n",
    "y_train = df_combined[df_combined['Year'].isin(train_years)]['LapTime']\n",
    "X_test = df_combined[df_combined['Year'] == test_year].drop(['LapTime', 'Year'], axis=1)\n",
    "y_test = df_combined[df_combined['Year'] == test_year]['LapTime']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drop rows where the target variable is missing in the training set\n",
    "train_indices = y_train.dropna().index  # Indices of rows where y_train is not NaN\n",
    "X_train = X_train.loc[train_indices]\n",
    "y_train = y_train.dropna()  # Drop missing values in y_train\n",
    "\n",
    "numeric_features = ['Humidity', 'Pressure', 'WindDirection', 'WindSpeed','TrackTemp','AirTemp','SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']\n",
    "\n",
    "X_train[numeric_features] = X_train[numeric_features].fillna(method='ffill')\n",
    "\n",
    "# Forward fill missing values in the test set\n",
    "X_test[numeric_features] = X_test[numeric_features].fillna(method='ffill')\n",
    "\n",
    "# Scale the test set using the same scaler fitted on the training set\n",
    "\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T14:32:10.502226Z",
     "start_time": "2024-05-28T14:32:08.313407Z"
    }
   },
   "id": "4ce7e476960b8bd4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/533f3cq93vq25vl2f3txpkb40000gn/T/ipykernel_1216/2388303341.py:106: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X_train[numeric_features] = X_train[numeric_features].fillna(method='ffill')\n",
      "/var/folders/_5/533f3cq93vq25vl2f3txpkb40000gn/T/ipykernel_1216/2388303341.py:109: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X_test[numeric_features] = X_test[numeric_features].fillna(method='ffill')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#1. Linear regression model with cross validation\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Initialize and fit Linear Regression\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "linear.fit(X_train, y_train)\n",
    "y_pred = linear.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
    "print(rmse)\n",
    "\n",
    "\n",
    "# Evaluate with K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(linear, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-scores)  # Convert MSE to RMSE\n",
    "print(\"Cross-validated RMSE scores:\", rmse_scores)\n",
    "print(\"Mean RMSE:\", np.mean(rmse_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T14:32:14.009240Z",
     "start_time": "2024-05-28T14:32:10.504017Z"
    }
   },
   "id": "ad25be2a9208ad48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.734507551293419\n",
      "Cross-validated RMSE scores: [3.42618768 3.50579253 3.44386084 3.45867339 3.47477468]\n",
      "Mean RMSE: 3.4618578236323736\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize and fit Random Forest with specified parameters\n",
    "rf_model = RandomForestRegressor(max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200, random_state=42,n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rf_rmse = mean_squared_error(y_test, rf_predictions, squared=False)\n",
    "print(f\"Random Forest RMSE: {rf_rmse:.3f}\")\n",
    "\n",
    "# Evaluate with K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-scores)  # Convert MSE to RMSE\n",
    "print(\"Cross-validated RMSE scores:\", rmse_scores)\n",
    "print(\"Mean RMSE:\", np.mean(rmse_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T14:34:07.733315Z",
     "start_time": "2024-05-28T14:32:14.022780Z"
    }
   },
   "id": "d8b328e44974afdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: 8.052\n",
      "Cross-validated RMSE scores: [1.84872689 1.85824514 1.87492179 1.86537882 1.76326769]\n",
      "Mean RMSE: 1.842108066210674\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##BASE XG BOOST\n",
    "\n",
    "#Grid value :(n_estimators=700, learning_rate=0.1,random_state=42,n_jobs=-1,max_depth=7)\n",
    "\n",
    "# Train the model with class weight adjustment\n",
    "xgb_model = XGBRegressor(random_state=42,reg_alpha=10,  # L1 regularization\n",
    "                         reg_lambda=1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "predictions = xgb_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Baseline RMSE for Combined df: {rmse}\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate with K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42,)\n",
    "scores = cross_val_score(xgb_model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-scores)  # Convert MSE to RMSE\n",
    "print(\"Cross-validated RMSE scores:\", rmse_scores)\n",
    "print(\"Mean RMSE:\", np.mean(rmse_scores))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T14:34:12.958315Z",
     "start_time": "2024-05-28T14:34:07.734196Z"
    }
   },
   "id": "ae43d91be68bb440",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE for Combined df: 5.365817002847865\n",
      "Cross-validated RMSE scores: [1.86925626 1.93032117 1.8025015  1.89152527 1.79397305]\n",
      "Mean RMSE: 1.8575154497159034\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "#4. XG boost with Random Search\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'reg_alpha': [0.1, 1, 10, 100],\n",
    "    'reg_lambda': [0.1, 1, 10, 100],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "neg_rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=True)\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=100, cv=kf,\n",
    "                                   scoring={'RMSE': neg_rmse_scorer}, refit='RMSE', random_state=42, verbose=3, n_jobs=-1)\n",
    "random_search.fit(X, y)\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_rmse = (-random_search.best_score_) ** 0.5  # Converting MSE to RMSE\n",
    "\n",
    "print(\"Best model parameters:\", best_params)\n",
    "print(f\"Optimized CV RMSE: {best_rmse:.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-28T14:34:12.959535Z"
    }
   },
   "id": "de182ec89fe241a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "93c9985a4716abc1"
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use SHAP to explain feature importance\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Calculate the mean absolute value for each feature to represent importance\n",
    "shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "feature_importance_shap = pd.Series(shap_sum, index=X_test.columns)\n",
    "\n",
    "# Scale the importances so that they sum to 100%\n",
    "feature_importance_shap_scaled = 100 * feature_importance_shap / feature_importance_shap.sum()\n",
    "\n",
    "# Print all scaled feature importances\n",
    "print(\"Feature importances scaled to 100%:\")\n",
    "for feature, importance in feature_importance_shap_scaled.sort_values(ascending=False).items():\n",
    "    print(f\"{feature}: {importance:.2f}%\")\n",
    "\n",
    "# Define feature groups\n",
    "weather_keywords = ['Pressure', 'Temp', 'Humidity', 'Rainfall', 'WindSpeed', 'WindDirection', 'Weather_Category', 'TrackTemp_Cat']\n",
    "weather_features = [col for col in X_test.columns if any(keyword in col for keyword in weather_keywords)]\n",
    "circuit_features = [col for col in X_test.columns if 'Circuit' in col]\n",
    "\n",
    "# Summarize weather impacts\n",
    "weather_importance = sum(importance for feature, importance in feature_importance_shap_scaled.items() if feature in weather_features)\n",
    "print(f\"Total importance of weather features scaled to 100%: {weather_importance:.2f}%\")\n",
    "\n",
    "# Summarize circuits impacts\n",
    "circuit_importance = sum(importance for feature, importance in feature_importance_shap_scaled.items() if feature in circuit_features)\n",
    "print(f\"Total importance of circuit features scaled to 100%: {circuit_importance:.2f}%\")\n",
    "\n",
    "# Plot feature importance using SHAP\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "747ab44f6adcad7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"# Assuming xgb_model has already been trained\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Creating a list to store feature names and their importances\n",
    "features_sorted = []\n",
    "for index in sorted_indices:\n",
    "    features_sorted.append((X_train.columns[index], feature_importances[index]))\n",
    "\n",
    "# Now features_sorted contains all the features and their importances sorted by importance\n",
    "weather_features = ['Pressure', 'AirTemp', 'Humidity', 'Rainfall', 'WindSpeed', 'TrackTemp', 'WindDirection']\n",
    "circuit_features = [col for col in X_train.columns if 'Circuit' in col]\n",
    "\n",
    "# Summarize weather impacts\n",
    "weather_importance = sum(importance for feature, importance in features_sorted if feature in weather_features)\n",
    "print(f\"Total importance of weather features: {weather_importance}\")\n",
    "\n",
    "# Summarize circuits impacts\n",
    "circuit_importance = sum(importance for feature, importance in features_sorted if feature in circuit_features)\n",
    "print(f\"Total importance of circuit features: {circuit_importance}\")\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "f8ca8a4f6e4b94ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate Spearman correlation matrix for df_combined\n",
    "correlation_matrix = df_combined.corr(method='spearman')\n",
    "\n",
    "# Focus on 'LapTime' correlations\n",
    "laptime_correlations = correlation_matrix['LapTime'].drop('LapTime')  # drop self-correlation\n",
    "\n",
    "# Convert correlations to percentage and sort in descending order\n",
    "laptime_correlations_percentage = laptime_correlations * 100\n",
    "laptime_correlations_sorted = laptime_correlations_percentage.sort_values(ascending=False)\n",
    "\n",
    "# Convert to list and format the output\n",
    "laptime_correlations_list = [(f\"{index}: {value:.2f}%\") for index, value in laptime_correlations_sorted.items()]\n",
    "\n",
    "# Print the formatted list\n",
    "for item in laptime_correlations_list:\n",
    "    print(item)\n",
    "\n",
    "# Optionally, visualize the correlations in descending order\n",
    "plt.figure(figsize=(10, 8))\n",
    "laptime_correlations_sorted.plot(kind='bar')\n",
    "plt.title('Percentage Correlation of LapTime with Other Variables')\n",
    "plt.ylabel('Correlation Percentage')\n",
    "plt.xlabel('Variables')\n",
    "plt.grid(True)\n",
    "plt.show()\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "cc8bb5edee8a7173",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
